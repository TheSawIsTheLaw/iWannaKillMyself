{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28d243e6",
   "metadata": {},
   "source": [
    "# Всякий потребный функционал для рисовашек"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "589e7f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "\n",
    "# sklearn\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Матрица ошибок',\n",
    "                          cmap=plt.cm.Blues,\n",
    "                          plot_place=[0, 0, 0]):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "\n",
    "    plt.subplot(*plot_place)\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title + \" # \" + str(plot_place[2]))\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('Истина')\n",
    "    plt.xlabel('Прогноз')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f08370c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "SPLIT_NUMBER = 4\n",
    "\n",
    "def confusionMatrices(vectorized_data, estimator, classes = []):\n",
    "    classes_data = data['class'].apply(lambda x: int(x))\n",
    "    splits = SPLIT_NUMBER\n",
    "    kf = KFold(n_splits=splits)\n",
    "    n = 1\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    for train, test in kf.split(vectorized_data):\n",
    "        estimator.fit(vectorized_data[train], classes_data.iloc[train].values.ravel())\n",
    "        predicted = estimator.predict(vectorized_data[test])\n",
    "\n",
    "        matrix = confusion_matrix(classes_data.iloc[test], predicted)\n",
    "        plot_confusion_matrix(matrix, classes, plot_place=[int(splits / 2) + splits % 2, 2, n])\n",
    "        n += 1\n",
    "\n",
    "def crossScores(vectorized_data, estimator):\n",
    "    classes_data = data['class'].apply(lambda x: int(x))\n",
    "    crossScoreAccuracy = cross_val_score(estimator, scoring='accuracy', X=vectorized_data, y=classes_data.tolist(), cv=SPLIT_NUMBER)\n",
    "    crossScoreF = cross_val_score(estimator, scoring='f1', X=vectorized_data, y=classes_data.tolist(), cv=SPLIT_NUMBER)\n",
    "    crossScoreRocAuc = cross_val_score(estimator, scoring='roc_auc', X=vectorized_data, y=classes_data.tolist(), cv=SPLIT_NUMBER)\n",
    "\n",
    "    fig, axs = plt.subplots(3, figsize=(8, 15))\n",
    "    \n",
    "    print(crossScoreAccuracy)\n",
    "    axs[0].plot(crossScoreAccuracy)\n",
    "    axs[0].set_title(\"Точность\")\n",
    "    axs[0].set(xlabel='Порядковый номер разбиения', ylabel='Значение метрики')\n",
    "\n",
    "    print(crossScoreF)\n",
    "    axs[1].plot(crossScoreF)\n",
    "    axs[1].set_title(\"F1\")\n",
    "    axs[1].set(xlabel='Порядковый номер разбиения', ylabel='Значение метрики')\n",
    "    \n",
    "    print(crossScoreRocAuc)\n",
    "    axs[2].plot(crossScoreRocAuc)\n",
    "    axs[2].set_title(\"ROC AUC\")\n",
    "    axs[2].set(xlabel='Порядковый номер разбиения', ylabel='Значение метрики')\n",
    "\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "def gridSearch(vectorized_data, estimator, paramGrid):\n",
    "    classes_data = data['class'].apply(lambda x: int(x))\n",
    "    \n",
    "    xTrain, xTest, yTrain, yTest = train_test_split(vectorized_data, classes_data, test_size=1 / SPLIT_NUMBER)\n",
    "\n",
    "    searchCV = GridSearchCV(estimator, param_grid=paramGrid, n_jobs=-1, scoring='f1', refit=False)\n",
    "    searchCV.fit(xTrain, yTrain)\n",
    "    print(searchCV.best_params_)\n",
    "    searchCV.cv_results_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb632db5",
   "metadata": {},
   "source": [
    "# Готовим покушать"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2bed5a0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>343</td>\n",
       "      <td>423</td>\n",
       "      <td>Ну все ебать!!! Завтра день радости будет!!!</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>476</td>\n",
       "      <td>476</td>\n",
       "      <td>Ещё два года назад я потеряла самого близкого ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>205</td>\n",
       "      <td>248</td>\n",
       "      <td>Ой, еще люблю, когда ешь что-то, что тебе хоче...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>522</td>\n",
       "      <td>651</td>\n",
       "      <td>в общем в любой непонятной ситуации я кидаю св...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>668</td>\n",
       "      <td>668</td>\n",
       "      <td>Сегодня все закончится. Я так больше не могу. ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>672</td>\n",
       "      <td>672</td>\n",
       "      <td>В моей стране очень тяжело достать оружие, одн...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>876</td>\n",
       "      <td>876</td>\n",
       "      <td>Или я начал сходить с ума , а может что-то или...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>608</td>\n",
       "      <td>760</td>\n",
       "      <td>знаете, я передумала. завтра все выучу к 7 уро...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>118</td>\n",
       "      <td>141</td>\n",
       "      <td>Онет меня забуллили ХАХАХАХАХХАХАХАХАХХАХА</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>395</td>\n",
       "      <td>395</td>\n",
       "      <td>Точка поставлена.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0.1  Unnamed: 0  \\\n",
       "0              343         423   \n",
       "1              476         476   \n",
       "2              205         248   \n",
       "3              522         651   \n",
       "4              668         668   \n",
       "...            ...         ...   \n",
       "1995           672         672   \n",
       "1996           876         876   \n",
       "1997           608         760   \n",
       "1998           118         141   \n",
       "1999           395         395   \n",
       "\n",
       "                                                   text  class  \n",
       "0          Ну все ебать!!! Завтра день радости будет!!!      0  \n",
       "1     Ещё два года назад я потеряла самого близкого ...      1  \n",
       "2     Ой, еще люблю, когда ешь что-то, что тебе хоче...      0  \n",
       "3     в общем в любой непонятной ситуации я кидаю св...      0  \n",
       "4     Сегодня все закончится. Я так больше не могу. ...      1  \n",
       "...                                                 ...    ...  \n",
       "1995  В моей стране очень тяжело достать оружие, одн...      1  \n",
       "1996  Или я начал сходить с ума , а может что-то или...      1  \n",
       "1997  знаете, я передумала. завтра все выучу к 7 уро...      0  \n",
       "1998         Онет меня забуллили ХАХАХАХАХХАХАХАХАХХАХА      0  \n",
       "1999                                  Точка поставлена.      1  \n",
       "\n",
       "[2000 rows x 4 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# from sklearn.utils import shuffle\n",
    "\n",
    "# suicidal = pd.read_csv(\"PreparedDatasets/suicidal.csv\")\n",
    "# non_suicidal = pd.read_csv(\"PreparedDatasets/non_suicidal.csv\").head(1000)\n",
    "\n",
    "# data = suicidal.append(non_suicidal)\n",
    "\n",
    "# data = shuffle(data)\n",
    "# data.to_csv(\"PreparedDatasets/shuffled.csv\")\n",
    "data = pd.read_csv(\"PreparedDatasets/shuffled.csv\")\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0fedfb",
   "metadata": {},
   "source": [
    "# Готовим векторные виды"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                   ебать ! ! ! завтра день радость ! ! !\n",
       "1       ещё год назад потерять близкий человек , сожал...\n",
       "2       ой , любить , что-то , хотеться . что-то сладк...\n",
       "3       общий любой непонятный ситуация кидать подруга чс\n",
       "4                  сегодня закончиться . мочь . уходить .\n",
       "                              ...                        \n",
       "1995    страна очень тяжело достать оружие , однако по...\n",
       "1996    начать сходить ум , что-то кто-то начать подав...\n",
       "1997    знать , передумать . завтра выучить 7 урок пер...\n",
       "1998                  онет забуллили хахахахаххахахахахха\n",
       "1999                                    точка поставить .\n",
       "Name: text, Length: 2000, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pymorphy3 import MorphAnalyzer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "an = MorphAnalyzer(lang='ru')\n",
    "stops = stopwords.words('russian')\n",
    "\n",
    "def getClearSentences(sentences):\n",
    "    return \" \".join(str(s) + \"\" for s in (an.normal_forms(y)[0] for y in filter(lambda x: x not in stops, nltk.word_tokenize(str(sentences)))))\n",
    "\n",
    "data['text'] = data['text'].apply(lambda x: getClearSentences(x))\n",
    "corpus = data['text']\n",
    "\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a05a03f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../TermoDemo/vectorizers/bagVectorizer.sav']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "vectorized_data_bag = vectorizer.fit_transform(corpus)\n",
    "\n",
    "import joblib\n",
    "\n",
    "joblib.dump(vectorizer, \"../TermoDemo/vectorizers/bagVectorizer.sav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "757c3e95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2000x83818 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 82007 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "from pandas import DataFrame\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('cointegrated/rubert-tiny2')\n",
    "\n",
    "bert_tokenized = corpus.apply(lambda ser: tokenizer.convert_tokens_to_ids(tokenizer.tokenize(ser)))\n",
    "bert_list = bert_tokenized.tolist()\n",
    "\n",
    "nRows = len(bert_list)\n",
    "nCols = max(max(row) if (len(row) > 0) else 0 for row in bert_list) + 1\n",
    "\n",
    "dataIn = []\n",
    "indices = []\n",
    "indptr = [0]\n",
    "\n",
    "for row in bert_list:\n",
    "    indices.extend(row)\n",
    "    dataIn.extend([1] * len(row))\n",
    "    indptr.append(len(indices))\n",
    "\n",
    "vectorized_data_bert = csr_matrix((dataIn, indices, indptr), shape=(nRows, nCols))\n",
    "vectorized_data_bert"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6015fe",
   "metadata": {},
   "source": [
    "# Облака слов и частотность"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64f0ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import dirt_tongue\n",
    "\n",
    "from string import punctuation\n",
    "\n",
    "detector = dirt_tongue.is_dirt()\n",
    "stops = stopwords.words('russian')\n",
    "dataCloud = data.copy()\n",
    "\n",
    "dataCloud['text'] = dataCloud['text'].apply(lambda x: nltk.word_tokenize(str(x).lower()))\n",
    "dataCloud['text'] = dataCloud['text'].apply(lambda z: list(filter(lambda x: x not in punctuation, z)))\n",
    "dataCloud['text'] = dataCloud['text'].apply(lambda z: list(filter(lambda x: x not in stops, z)))\n",
    "dataCloud['text'] = dataCloud['text'].apply(lambda z: list(map(lambda x: x if not detector(x) else x[0] + ('*' * (len(x) - 2)) + x[-1], z)))\n",
    "\n",
    "wordsCloudText = ' '.join(dataCloud[dataCloud['class'] == 1]['text'].apply(lambda x: ' '.join(x)))\n",
    "\n",
    "print(nltk.FreqDist(wordsCloudText.split()).most_common(10))\n",
    "\n",
    "cloud = WordCloud(width=1500, height=750, regexp=r\"\\w[\\w*']+\").generate(wordsCloudText)\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.imshow(cloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.savefig(\"plots/cloudSuicidal.pdf\", format=\"pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa528090",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import dirt_tongue\n",
    "\n",
    "from string import punctuation\n",
    "\n",
    "detector = dirt_tongue.is_dirt()\n",
    "stops = stopwords.words('russian')\n",
    "dataCloud = data.copy()\n",
    "\n",
    "dataCloud['text'] = dataCloud['text'].apply(lambda x: nltk.word_tokenize(str(x).lower()))\n",
    "dataCloud['text'] = dataCloud['text'].apply(lambda z: list(filter(lambda x: x not in punctuation, z)))\n",
    "dataCloud['text'] = dataCloud['text'].apply(lambda z: list(filter(lambda x: x not in stops, z)))\n",
    "dataCloud['text'] = dataCloud['text'].apply(lambda z: list(map(lambda x: x if not detector(x) else x[0] + ('*' * (len(x) - 2)) + x[-1], z)))\n",
    "\n",
    "wordsCloudText = ' '.join(dataCloud[dataCloud['class'] == 0]['text'].apply(lambda x: ' '.join(x)))\n",
    "\n",
    "print(nltk.FreqDist(wordsCloudText.split()).most_common(10))\n",
    "\n",
    "cloud = WordCloud(width=1500, height=750, regexp=r\"\\w[\\w*']+\").generate(wordsCloudText)\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.imshow(cloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.savefig(\"plots/cloudNonSuicidal.pdf\", format=\"pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884a2616",
   "metadata": {},
   "source": [
    "# Сантименты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fff2e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dostoevsky.tokenization import RegexTokenizer\n",
    "from dostoevsky.models import FastTextSocialNetworkModel\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def sentimentsDiagram(to_analyze):\n",
    "    tokenizer = RegexTokenizer()\n",
    "    model = FastTextSocialNetworkModel(tokenizer=tokenizer)\n",
    "\n",
    "    # Get the sentiment analysis results for the corpus\n",
    "    results = model.predict(to_analyze, k=2)\n",
    "\n",
    "    # Define a function to get the sentiment score of a message\n",
    "    def get_sentiment_score(sentiment_dict):\n",
    "        if 'positive' in sentiment_dict:\n",
    "            return sentiment_dict['positive']\n",
    "        elif 'negative' in sentiment_dict:\n",
    "            return -sentiment_dict['negative']\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    # Calculate the sentiment scores for the corpus\n",
    "    sentiment_scores = [get_sentiment_score(sentiment) for sentiment in results]\n",
    "\n",
    "    # Calculate the average sentiment score\n",
    "    average_sentiment = sum(sentiment_scores) / len(sentiment_scores)\n",
    "\n",
    "    # Count the number of positive, neutral, and negative sentiments\n",
    "    positive = sum(1 for score in sentiment_scores if score > 0)\n",
    "    neutral = sum(1 for score in sentiment_scores if score == 0)\n",
    "    negative = sum(1 for score in sentiment_scores if score < 0)\n",
    "\n",
    "    # Create a pie chart to visualize the sentiments\n",
    "    labels = ['Позитивный', 'Нейтральный', 'Отрицательный']\n",
    "    sizes = [positive, neutral, negative]\n",
    "    colors = ['#5cb85c', '#f0ad4e', '#d9534f']\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    patches, texts = plt.pie(sizes, colors=colors, startangle=90)\n",
    "    plt.legend(patches, labels, loc=\"best\")\n",
    "    plt.axis('equal')\n",
    "    plt.title('Анализ настроения\\nСреднее настроение: {:.2f}'.format(average_sentiment))\n",
    "    plt.savefig(\"plots/sentiments_suicidal.pdf\", format=\"pdf\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c659204",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentimentsDiagram(data[data['class'] == 0]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2db2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentimentsDiagram(data[data['class'] == 1]['text'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ee5066cc",
   "metadata": {},
   "source": [
    "# Градиентный бустинг"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155b8079",
   "metadata": {},
   "source": [
    "## Bag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67df9fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "estimator = GradientBoostingClassifier()\n",
    "gridSearch(vectorized_data_bag, estimator, paramGrid={'learning_rate': [0.5], 'min_samples_split': [4, 5], 'n_estimators': [210, 215, 220]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b17da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "estimator = GradientBoostingClassifier(learning_rate=0.5, min_samples_split=5, n_estimators=215)\n",
    "confusionMatrices(vectorized_data_bag, estimator=estimator, classes=[\"суицидальное\", \"обычное\"])\n",
    "plt.savefig(\"plots/gradientMatrBag.pdf\", format=\"pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8f417b",
   "metadata": {},
   "outputs": [],
   "source": [
    "crossScores(vectorized_data_bag, estimator)\n",
    "plt.savefig(\"plots/gradientMetricsBag.pdf\", format=\"pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b96e36be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['models/gradientBag.sav']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimator = GradientBoostingClassifier(learning_rate=0.5, min_samples_split=5, n_estimators=215)\n",
    "estimator.fit(vectorized_data_bag, data['class'].apply(lambda x: int(x)))\n",
    "\n",
    "import joblib\n",
    "joblib.dump(estimator, \"models/gradientBag.sav\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09c8f8c",
   "metadata": {},
   "source": [
    "## BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a4142b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "estimator = GradientBoostingClassifier()\n",
    "gridSearch(vectorized_data_bert, estimator, paramGrid={'learning_rate': [0.3], 'min_samples_split': [16], 'n_estimators': [245]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e55f6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "estimator = GradientBoostingClassifier(learning_rate=0.3, min_samples_split=16, n_estimators=245)\n",
    "confusionMatrices(vectorized_data_bert, estimator=estimator, classes=[\"суицидальное\", \"обычное\"])\n",
    "plt.savefig(\"plots/gradientMatrBert.pdf\", format=\"pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6adbc9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "crossScores(vectorized_data_bert, estimator)\n",
    "plt.savefig(\"plots/gradientMetricsBert.pdf\", format=\"pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "be2bc4a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['models/gradientBert.sav']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimator = GradientBoostingClassifier(learning_rate=0.3, min_samples_split=16, n_estimators=245)\n",
    "estimator.fit(vectorized_data_bert, data['class'].apply(lambda x: int(x)))\n",
    "\n",
    "import joblib\n",
    "joblib.dump(estimator, \"../TermoDemo/models/gradientBert.sav\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "68048634",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dce74aa",
   "metadata": {},
   "source": [
    "## Bag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e089f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "estimator = RandomForestClassifier()\n",
    "gridSearch(vectorized_data_bag, estimator, paramGrid={'n_jobs': [-1],'n_estimators': [850, 900, 950], 'max_depth':[300, None], 'max_features': ['sqrt', 'log2', None]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7486646e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "estimator = RandomForestClassifier(max_depth=300, max_features='log2', n_estimators=850, n_jobs=-1)\n",
    "confusionMatrices(vectorized_data_bag, estimator, [\"суицидальное\", \"обычное\"])\n",
    "plt.savefig(\"plots/randomMatrBag.pdf\", format=\"pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04afd97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "crossScores(vectorized_data_bag, estimator)\n",
    "plt.savefig(\"plots/randomMetricsBag.pdf\", format=\"pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "182cc23f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../TermoDemo/models/randomBag.sav']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "estimator = RandomForestClassifier(max_depth=300, max_features='log2', n_estimators=850, n_jobs=-1)\n",
    "estimator.fit(vectorized_data_bag, data['class'].apply(lambda x: int(x)))\n",
    "\n",
    "import joblib\n",
    "joblib.dump(estimator, \"../TermoDemo/models/randomBag.sav\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada65bd8",
   "metadata": {},
   "source": [
    "## BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1195f196",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "estimator = RandomForestClassifier()\n",
    "gridSearch(vectorized_data_bert, estimator, paramGrid={'n_jobs': [-1],'n_estimators': [2550, 2600, 2650], 'max_depth':[None], 'max_features': ['log2']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a9f340",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "estimator = RandomForestClassifier(max_features='log2', n_estimators=2600, n_jobs=-1)\n",
    "plt.figure(figsize=(20, 10))\n",
    "confusionMatrices(vectorized_data_bert, estimator, [\"суицидальное\", \"обычное\"])\n",
    "plt.savefig(\"plots/randomMatrBert.pdf\", format=\"pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7e9d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "crossScores(vectorized_data_bert, estimator)\n",
    "plt.savefig(\"plots/randomMetricsBert.pdf\", format=\"pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d53decd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "estimator = RandomForestClassifier(max_features='log2', n_estimators=2600, n_jobs=-1)\n",
    "estimator.fit(vectorized_data_bert, data['class'].apply(lambda x: int(x)))\n",
    "\n",
    "import joblib\n",
    "joblib.dump(estimator, \"../TermoDemo/models/randomBert.sav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b661672b",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_corp = corpus.append(pd.Series([getClearSentences(\"хочу напиться вусмерть\")]))\n",
    "\n",
    "bert_tokenized = temp_corp.apply(lambda ser: tokenizer.convert_tokens_to_ids(tokenizer.tokenize(ser)))\n",
    "bert_list = bert_tokenized.tolist()\n",
    "\n",
    "nRows = len(bert_list)\n",
    "nCols = max(max(row) if (len(row) > 0) else 0 for row in bert_list) + 1\n",
    "\n",
    "dataIn = []\n",
    "indices = []\n",
    "indptr = [0]\n",
    "\n",
    "for row in bert_list:\n",
    "    indices.extend(row)\n",
    "    dataIn.extend([1] * len(row))\n",
    "    indptr.append(len(indices))\n",
    "\n",
    "vectorized_temp = csr_matrix((dataIn, indices, indptr), shape=(nRows, nCols))\n",
    "estimator.predict(vectorized_temp[-1:])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0d1d35bf",
   "metadata": {},
   "source": [
    "# SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f64bd9",
   "metadata": {},
   "source": [
    "## Bag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ab702a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "estimator = SVC()\n",
    "gridSearch(vectorized_data_bag, estimator, paramGrid={'C': [7.0], 'degree': [2, 3, 4, 6, 7], 'kernel': ['linear', 'poly', 'rbf']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401bb028",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = SVC(C=7.0, degree=2, kernel='linear')\n",
    "confusionMatrices(vectorized_data_bag, estimator, [\"суицидальное\", \"обычное\"])\n",
    "plt.savefig(\"plots/svcMatrBag.pdf\", format=\"pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a96336",
   "metadata": {},
   "outputs": [],
   "source": [
    "crossScores(vectorized_data_bag, estimator)\n",
    "plt.savefig(\"plots/svcMetricsBag.pdf\", format=\"pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0110213f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['models/svcBag.sav']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "estimator = SVC(C=7.0, degree=2, kernel='linear')\n",
    "estimator.fit(vectorized_data_bag, data['class'].apply(lambda x: int(x)))\n",
    "\n",
    "import joblib\n",
    "joblib.dump(estimator, \"../TermoDemo/models/svcBag.sav\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0b4f38",
   "metadata": {},
   "source": [
    "## BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33f8c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "estimator = SVC()\n",
    "gridSearch(vectorized_data_bert, estimator, paramGrid={'C': [1.5, 2.0, 2.5], 'degree': [2, 3, 4, 5, 6, 7], 'kernel': ['linear', 'poly', 'rbf']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5001156d",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = SVC(C=1.5, degree=2, kernel='linear')\n",
    "confusionMatrices(vectorized_data_bert, estimator, [\"суицидальное\", \"обычное\"])\n",
    "plt.savefig(\"plots/svcMatrBert.pdf\", format=\"pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c968f342",
   "metadata": {},
   "outputs": [],
   "source": [
    "crossScores(vectorized_data_bert, estimator)\n",
    "plt.savefig(\"plots/svcMetricsBert.pdf\", format=\"pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "79ca4a01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['models/svcBert.sav']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimator = SVC(C=1.5, degree=2, kernel='linear')\n",
    "estimator.fit(vectorized_data_bert, data['class'].apply(lambda x: int(x)))\n",
    "\n",
    "import joblib\n",
    "joblib.dump(estimator, \"../TermoDemo/models/svcBert.sav\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf4add78",
   "metadata": {},
   "source": [
    "# KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad1f9a9c",
   "metadata": {},
   "source": [
    "## Bag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7042b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "estimator = KNeighborsClassifier()\n",
    "print(gridSearch(vectorized_data_bag, estimator, paramGrid={'n_neighbors': [5, 6, 7], 'weights': ['uniform', 'distance'], 'leaf_size': [4, 5, 6], 'p': [1], 'metric': ['euclidean', 'manhattan']}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4bde4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = KNeighborsClassifier(n_neighbors=5, weights='distance', leaf_size=4, p=1, metric='euclidean')\n",
    "confusionMatrices(vectorized_data_bag, estimator, [\"суицидальное\", \"обычное\"])\n",
    "plt.savefig(\"plots/knnMatrBag.pdf\", format=\"pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085e8c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "crossScores(vectorized_data_bag, estimator)\n",
    "plt.savefig(\"plots/knnMetricsBag.pdf\", format=\"pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c384c8bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['models/knnBag.sav']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "estimator = KNeighborsClassifier(n_neighbors=5, weights='distance', leaf_size=4, p=1, metric='euclidean')\n",
    "estimator.fit(vectorized_data_bag, data['class'].apply(lambda x: int(x)))\n",
    "\n",
    "import joblib\n",
    "joblib.dump(estimator, \"../TermoDemo/models/knnBag.sav\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba9551a",
   "metadata": {},
   "source": [
    "## BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dad5402",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "estimator = KNeighborsClassifier()\n",
    "print(gridSearch(vectorized_data_bert, estimator, paramGrid={'n_neighbors': [1, 2, 3, 4, 5, 6, 7], 'weights': ['uniform', 'distance'], 'leaf_size': [1, 2, 3, 4, 5], 'p': [1, 2, 3, 4, 5], 'metric': ['euclidean', 'manhattan']}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eed85d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = KNeighborsClassifier(n_neighbors=6, weights='distance', leaf_size=1, p=1, metric='euclidean')\n",
    "confusionMatrices(vectorized_data_bert, estimator, [\"суицидальное\", \"обычное\"])\n",
    "plt.savefig(\"plots/knnMatrBert.pdf\", format=\"pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24de61ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "crossScores(vectorized_data_bert, estimator)\n",
    "plt.savefig(\"plots/knnMetricsBert.pdf\", format=\"pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2de2f52f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['models/knnBert.sav']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimator = KNeighborsClassifier(n_neighbors=6, weights='distance', leaf_size=1, p=1, metric='euclidean')\n",
    "estimator.fit(vectorized_data_bert, data['class'].apply(lambda x: int(x)))\n",
    "\n",
    "import joblib\n",
    "joblib.dump(estimator, \"../TermoDemo/models/knnBert.sav\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f8ca40",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c1a541b",
   "metadata": {},
   "source": [
    "## Bag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf1e59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "estimator = LogisticRegression()\n",
    "gridSearch(vectorized_data_bag, estimator, paramGrid={'penalty': ['l2'], 'C': [2.1, 2.2, 2.3, 2.4], 'class_weight': [{0: 1, 1: 2}, {0: 2, 1: 1}, 'balanced', None], 'solver': ['lbfgs', 'liblinear', 'newton-ct']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f63c614",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = LogisticRegression(penalty='l2', C= 2.2, class_weight='balanced', solver='liblinear')\n",
    "confusionMatrices(vectorized_data_bag, estimator, [\"суицидальное\", \"обычное\"])\n",
    "plt.savefig(\"plots/logicMatrBag.pdf\", format=\"pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25580eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "crossScores(vectorized_data_bag, estimator)\n",
    "plt.savefig(\"plots/logicMetricsBag.pdf\", format=\"pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e5f10718",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['models/logisticBag.sav']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "estimator = LogisticRegression(penalty='l2', C= 2.2, class_weight='balanced', solver='liblinear')\n",
    "estimator.fit(vectorized_data_bag, data['class'].apply(lambda x: int(x)))\n",
    "\n",
    "import joblib\n",
    "joblib.dump(estimator, \"../TermoDemo/models/logisticBag.sav\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2972996a",
   "metadata": {},
   "source": [
    "## BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6532ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "estimator = LogisticRegression()\n",
    "gridSearch(vectorized_data_bert, estimator, paramGrid={'penalty': ['l2'], 'C': [2.1, 2.2, 2.3, 2.4], 'class_weight': [{0: 1, 1: 2}, {0: 2, 1: 1}, 'balanced', None], 'solver': ['liblinear', 'newton-ct']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ef5d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = LogisticRegression(penalty='l2', C= 2.1, class_weight='balanced', solver='liblinear')\n",
    "confusionMatrices(vectorized_data_bert, estimator, [\"суицидальное\", \"обычное\"])\n",
    "plt.savefig(\"plots/logicMatrBert.pdf\", format=\"pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2a58b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "crossScores(vectorized_data_bert, estimator)\n",
    "plt.savefig(\"plots/logicMetricsBert.pdf\", format=\"pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9584e262",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['models/logisticBert.sav']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimator = LogisticRegression(penalty='l2', C= 2.1, class_weight='balanced', solver='liblinear')\n",
    "estimator.fit(vectorized_data_bert, data['class'].apply(lambda x: int(x)))\n",
    "\n",
    "import joblib\n",
    "joblib.dump(estimator, \"../TermoDemo/models/logisticBert.sav\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20218875",
   "metadata": {},
   "source": [
    "# Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c389c1",
   "metadata": {},
   "source": [
    "## Bag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3273cd4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "estimator = Perceptron()\n",
    "gridSearch(vectorized_data_bag, estimator, paramGrid={'penalty': ['l2', 'l1', 'elasticnet'], 'alpha': [0.0001, 0.0005, 0.001], 'n_jobs': [-1], 'max_iter': [30], 'class_weight': [{0:1, 1:2}, {0:2, 1:1}, 'balanced', None]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2d293f",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = Perceptron(alpha=0.0001, class_weight=None, max_iter=30, penalty='l1', n_jobs=-1)\n",
    "confusionMatrices(vectorized_data_bag, estimator, [\"суицидальное\", \"обычное\"])\n",
    "plt.savefig(\"plots/perceptronMatrBag.pdf\", format=\"pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab7c8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "crossScores(vectorized_data_bag, estimator)\n",
    "plt.savefig(\"plots/perceptronMetricsBag.pdf\", format=\"pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "497884ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['models/perceptronBag.sav']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "estimator = Perceptron(alpha=0.0001, class_weight=None, max_iter=30, penalty='l1', n_jobs=-1)\n",
    "estimator.fit(vectorized_data_bag, data['class'].apply(lambda x: int(x)))\n",
    "\n",
    "import joblib\n",
    "joblib.dump(estimator, \"../TermoDemo/models/perceptronBag.sav\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d528bb61",
   "metadata": {},
   "source": [
    "## BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8479b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "estimator = Perceptron()\n",
    "gridSearch(vectorized_data_bert, estimator, paramGrid={'penalty': ['l2', 'l1', 'elasticnet'], 'alpha': [0.0001, 0.0005, 0.001], 'n_jobs': [-1], 'max_iter': [30, 40, 50, 60, 100, 200, 300, 400], 'class_weight': [{0:1, 1:2}, {0:2, 1:1}, 'balanced', None]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f763e1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = Perceptron(alpha=0.0001, class_weight=None, max_iter=30, penalty='l1', n_jobs=-1)\n",
    "confusionMatrices(vectorized_data_bert, estimator, [\"суицидальное\", \"обычное\"])\n",
    "plt.savefig(\"plots/perceptronMatrBert.pdf\", format=\"pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296d924d",
   "metadata": {},
   "outputs": [],
   "source": [
    "crossScores(vectorized_data_bert, estimator)\n",
    "plt.savefig(\"plots/perceptronMetricsBert.pdf\", format=\"pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1d6c0e70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['models/perceptronBert.sav']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimator = Perceptron(alpha=0.0001, class_weight=None, max_iter=30, penalty='l1', n_jobs=-1)\n",
    "estimator.fit(vectorized_data_bert, data['class'].apply(lambda x: int(x)))\n",
    "\n",
    "import joblib\n",
    "joblib.dump(estimator, \"../TermoDemo/models/perceptronBert.sav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b3417e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
